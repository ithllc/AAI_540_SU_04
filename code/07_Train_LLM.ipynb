{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e736b8f5-5aee-47be-a27b-1a234459caab",
   "metadata": {},
   "source": [
    "# Fine tune Mixtral-8x7B with QLoRA and SageMaker remote decorator \n",
    "We are using SageMaker remote decorator for running the fine-tuning job on Amazon SageMaker Training job\n",
    "SageMaker Studio Kernel: PyTorch 2.0.0 Python 3.10\n",
    "\n",
    "JupyterLab Instance Type: ml.t3.medium\n",
    "\n",
    "Fine-Tuning:\n",
    "\n",
    "Instance Type: ml.g5.12xlarge\n",
    "\n",
    "Install the required libriaries, including the Hugging Face libraries, and restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33326499-f066-4f61-b859-fdc5df7c95e0",
   "metadata": {},
   "source": [
    "### Install Proper Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a1bd99-5583-49c2-a03f-8fc2ec41e2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.40.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.41.2)\n",
      "Requirement already satisfied: peft>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.11.1)\n",
      "Requirement already satisfied: accelerate>=0.29.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.31.0)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.43.1)\n",
      "Requirement already satisfied: evaluate>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.4.3)\n",
      "Requirement already satisfied: sagemaker>=2.220.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.222.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.19.1)\n",
      "Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.40.0->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft>=0.10.0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft>=0.10.0->-r requirements.txt (line 2)) (2.0.0.post101)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (2023.6.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate>=0.4.1->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.34.51)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (6.10.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.3.2)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.7.7)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (4.2.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.26.18)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker>=2.220.0->-r requirements.txt (line 7)) (7.0.0)\n",
      "Requirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (3.20.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>=4.40.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.220.0->-r requirements.txt (line 7)) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.40.0->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.10.0->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.10.0->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.10.0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate>=0.4.1->-r requirements.txt (line 5)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate>=0.4.1->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate>=0.4.1->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.220.0->-r requirements.txt (line 7)) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker>=2.220.0->-r requirements.txt (line 7)) (0.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate>=0.4.1->-r requirements.txt (line 5)) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft>=0.10.0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft>=0.10.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5962a29-b90d-49d3-a8e0-1ccef8e34ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U datasets>=2.18.0\n",
    "%pip install -q -U scikit-learn\n",
    "%pip install -q -U awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25ff59-4102-4984-bcad-59bc7dbc7fbd",
   "metadata": {},
   "source": [
    "### Import most of the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7ed77c-4cce-4a9d-8e13-61c7291ca0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12913/3209399217.py:10: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283db6c3-e485-4b17-9903-f9f32768f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket_name                                       -> 'wizard-of-tasks-dataset-54321'\n",
      "ingest_create_athena_db_passed                    -> True\n",
      "ingest_create_athena_table_tsv_passed             -> True\n",
      "s3_private_data_path_csv                          -> 's3://sagemaker-us-east-1-114106928417/aai-540-2-1\n",
      "s3_private_path_tsv                               -> 's3://sagemaker-us-east-1-114106928417/amazon-revi\n",
      "s3_public_data_path_csv                           -> '/home/sagemaker-user/aai-540-homework/homework-2-\n",
      "s3_public_path_tsv                                -> 's3://dsoaws/amazon-reviews-pds/tsv'\n",
      "setup_dependencies_passed                         -> True\n",
      "setup_s3_bucket_passed                            -> True\n"
     ]
    }
   ],
   "source": [
    "# check dependencies are stored\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8d59fc-858c-437b-8cea-de15da3a33b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Amazon information\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "s3 = boto3.client('s3', region_name=sagemaker_session.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7061f4fd-8e9f-4086-932b-e57aec37c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://wizard-of-tasks-dataset-54321/data/train_df.csv\n"
     ]
    }
   ],
   "source": [
    "# get s3 path to data from stored variable\n",
    "%store -r bucket_name\n",
    "s3_train_dataset_path = 's3://{}/data/train_df.csv'.format(bucket_name)\n",
    "print(s3_train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04a20aa-3511-495b-a6b3-006c364ef10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://wizard-of-tasks-dataset-54321/data/val_df.csv\n"
     ]
    }
   ],
   "source": [
    "s3_validation_dataset_path = 's3://{}/data/val_df.csv'.format(bucket_name)\n",
    "print(s3_validation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "411ccf25-5c75-4e3a-983b-64fb9ce1932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://wizard-of-tasks-dataset-54321/data/test_df.csv\n"
     ]
    }
   ],
   "source": [
    "s3_test_dataset_path = 's3://{}/data/test_df.csv'.format(bucket_name)\n",
    "print(s3_test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd05870-a531-4fb1-89f2-4af5f695006b",
   "metadata": {},
   "source": [
    "### Setup Configuration file path\n",
    "We are setting the directory in which the config.yaml file resides so that remote decorator can make use of the settings through SageMaker Defaults.\n",
    "\n",
    "This notebook is using the Hugging Face container for the us-east-1 region. Make sure you are using the right image for your AWS region, otherwise edit config.yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "504df7bf-1c52-4e2c-8670-aeb00ba4652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897a4a1-5ace-42c1-bcdc-9cdebb3a22f9",
   "metadata": {},
   "source": [
    "### Import the data from S3 into Pandas Dataframe\n",
    "1. using aws wrangler\n",
    "2. review the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80dc6599-3a13-40ce-b364-6815811abb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_data = wr.s3.read_csv(path=s3_train_dataset_path, sep='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58c895f-c041-42fe-ae09-8eb1c47041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_data = wr.s3.read_csv(path=s3_validation_dataset_path, sep='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85dbe07e-fde2-4d12-b891-1aea76832085",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_data = wr.s3.read_csv(path=s3_test_dataset_path, sep='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe83ed7-c207-4f08-af06-c7ac5b99364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   question               404 non-null    object\n",
      " 1   intent_question        404 non-null    object\n",
      " 2   history                364 non-null    object\n",
      " 3   conversation_id        404 non-null    object\n",
      " 4   document_url_question  404 non-null    object\n",
      " 5   domain_question        404 non-null    object\n",
      " 6   text_answer            404 non-null    object\n",
      " 7   intent_answer          404 non-null    object\n",
      " 8   domain_answer          404 non-null    object\n",
      " 9   question_id            404 non-null    object\n",
      " 10  title                  404 non-null    object\n",
      " 11  description            404 non-null    object\n",
      " 12  ingredients            191 non-null    object\n",
      " 13  steps                  404 non-null    object\n",
      " 14  data_split             404 non-null    object\n",
      "dtypes: object(15)\n",
      "memory usage: 47.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7ee2cf-e631-417f-9885-40886d239bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271 entries, 0 to 270\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   question               271 non-null    object\n",
      " 1   intent_question        271 non-null    object\n",
      " 2   history                240 non-null    object\n",
      " 3   conversation_id        271 non-null    object\n",
      " 4   document_url_question  271 non-null    object\n",
      " 5   domain_question        271 non-null    object\n",
      " 6   text_answer            271 non-null    object\n",
      " 7   intent_answer          271 non-null    object\n",
      " 8   domain_answer          271 non-null    object\n",
      " 9   question_id            271 non-null    object\n",
      " 10  title                  271 non-null    object\n",
      " 11  description            271 non-null    object\n",
      " 12  ingredients            117 non-null    object\n",
      " 13  steps                  271 non-null    object\n",
      " 14  data_split             271 non-null    object\n",
      "dtypes: object(15)\n",
      "memory usage: 31.9+ KB\n"
     ]
    }
   ],
   "source": [
    "val_df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcca2b15-89c6-488d-9496-a1014d8cf5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 653 entries, 0 to 652\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   question               653 non-null    object\n",
      " 1   intent_question        653 non-null    object\n",
      " 2   history                583 non-null    object\n",
      " 3   conversation_id        653 non-null    object\n",
      " 4   document_url_question  653 non-null    object\n",
      " 5   domain_question        653 non-null    object\n",
      " 6   text_answer            653 non-null    object\n",
      " 7   intent_answer          653 non-null    object\n",
      " 8   domain_answer          653 non-null    object\n",
      " 9   question_id            653 non-null    object\n",
      " 10  title                  653 non-null    object\n",
      " 11  description            644 non-null    object\n",
      " 12  ingredients            262 non-null    object\n",
      " 13  steps                  653 non-null    object\n",
      " 14  data_split             653 non-null    object\n",
      "dtypes: object(15)\n",
      "memory usage: 76.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d51571c2-f48d-4c79-bb31-0ca8a8266d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                      question            intent_question                                                                                                                                                                                                                                                                                                                                                                                                                                    history       conversation_id                               document_url_question domain_question                                                                                                                              text_answer                 intent_answer domain_answer question_id                              title                                                                                                                                                                                                                                                                                                                                                 description ingredients                                                                                                                                                                                                                                                                                                                                                                                                                                          steps data_split\\n0         I'm not sure if you understood my previous question.  Could you try answering again?  ask_question_recipe_steps                                                                                                                                                                                                                                                                                                               student: Is the cheapest way to fertilize a garden with compost from my kitchen? | teacher: Cheap, convenient and efficient.  Wizard-of-Task-diy-9  https://www.wikihow.com/Fertilize-a-Garden-Cheaply             diy  There are many types of worms, two that are commonly raised are night crawlers and red wrigglers. I've shared details about worms here.  answer_question_recipe_steps           diy     diy-9-1  How to Fertilize a Garden Cheaply  Fertilizing your garden is a great way to improve the soil and ensure your plants grow well. Commercial fertilizer can be expensive and full of harmful chemicals that are toxic for the soil. You can fertilize your garden cheaply, and naturally, by using food products and plants. You can also use animal products as natural, cheap fertilizer.\\\\n\\\\n         NaN  ['Use coffee grounds as fertilizer.', 'Fertilize the garden with bananas.', 'Add egg shells to the garden.', 'Fertilize the garden with Blackstrap molasses.', 'Soak the garden with epsom salts.', 'Use grass clippings.', 'Make green manure with plants.', 'Use wood ash or sawdust as fertilizer.', 'Fertilize the garden with aged animal manure.', 'Use fish guts and bones to make fish fertilizer.', 'Fertilize with worm castings.']      train\\n1  I don't have a coffee grinder. Can I crush the shells by hand? How fine do they have to be?  ask_question_recipe_steps  student: We'll go with coffee, because I always have that around the house!  So how do I get started fertilizing? | teacher: You will begin by drying your coffee grounds before spreading them like mulch. I have added on cautions that you should take. | student: That's so helpful!  I've dried the grounds and spread them around.  Is that it? | teacher: It is fine but you could also try using egg shells for added nutrients.   Wizard-of-Task-diy-9  https://www.wikihow.com/Fertilize-a-Garden-Cheaply             diy                                                                                      You'll want to feed the worms various food scraps.   answer_question_recipe_steps           diy     diy-9-2  How to Fertilize a Garden Cheaply  Fertilizing your garden is a great way to improve the soil and ensure your plants grow well. Commercial fertilizer can be expensive and full of harmful chemicals that are toxic for the soil. You can fertilize your garden cheaply, and naturally, by using food products and plants. You can also use animal products as natural, cheap fertilizer.\\\\n\\\\n         NaN  ['Use coffee grounds as fertilizer.', 'Fertilize the garden with bananas.', 'Add egg shells to the garden.', 'Fertilize the garden with Blackstrap molasses.', 'Soak the garden with epsom salts.', 'Use grass clippings.', 'Make green manure with plants.', 'Use wood ash or sawdust as fertilizer.', 'Fertilize the garden with aged animal manure.', 'Use fish guts and bones to make fish fertilizer.', 'Fertilize with worm castings.']      train\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_data.head(2).to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86295b-fd04-4747-a12c-34fadfc0beec",
   "metadata": {},
   "source": [
    "### Create a prompt and load the dataset to try question and answering\n",
    "1. function takes the document_url_question and creates a domain from it\n",
    "2. applies the columns from the datasets to the prompt template\n",
    "\n",
    "Prompt template designed to apply as much context as needed from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130dff1d-b901-40f0-9f7f-c152e5626850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "def template_dataset(sample):\n",
    "    # Check if 'document_url_question' contains 'wholefoods'\n",
    "    if 'wholefoods' in sample['document_url_question']:\n",
    "        domain = 'food'\n",
    "    else:\n",
    "        domain = 'diy'\n",
    "\n",
    "    title = \"\"\n",
    "    description = \"\"\n",
    "    ingredients = \"\"\n",
    "    steps = \"\"\n",
    "    data_split = \"\"\n",
    "\n",
    "    prompt_template  = f\"\"\"\\\n",
    "    <s>[INST]\n",
    "    Question: {title} [/INST]\n",
    "    Context: {description}. For this task, you will need the following: {', '.join(ingredients)}. This is a {domain} task.\n",
    "    Answer: Please provide a step-by-step guide. {', '.join(steps)}</s>\n",
    "    [INST]\n",
    "    Data Split: This is a {data_split} example for the {domain} task.\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "    \n",
    "    sample[\"text\"] = prompt_template.format(title=sample['title'], \n",
    "                                            description=sample['description'], \n",
    "                                            ingredients=sample['ingredients'], \n",
    "                                            steps=sample['steps'], \n",
    "                                            domain=domain, \n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96edf49d-c7d5-4680-ba8e-4081a888aafd",
   "metadata": {},
   "source": [
    "### Use Huggingface to import the Mixtral8x7B model\n",
    "\n",
    "Also using it to import the the Hugging Face Trainer class to fine-tune the model. Define the hyperparameters we want to use. We also create a DataCollator that will take care of padding our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1557fe4e-3e55-4f3c-91bf-f8b0be8b959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d66a74-5fca-466d-8a18-f958f5022ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/sagemaker-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token {access_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f291703-eb73-44b5-b5f5-a9f90b13c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653247ee-bb6b-4008-8d6f-6d9ce9eded03",
   "metadata": {},
   "source": [
    "### Apply the Train, Validation, and Test Datasets to HugginFace Dataset Library and apply the prompt template to the required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc0320ec-6e7e-4f76-acc8-04529d0d2f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83074ae747d4253aa1345087add3049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    <s>[INST]\n",
      "    Question:  [/INST]\n",
      "    Context: . For this task, you will need the following: . This is a diy task.\n",
      "    Answer: Please provide a step-by-step guide. </s>\n",
      "    [INST]\n",
      "    Data Split: This is a  example for the diy task.\n",
      "    [/INST]\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d82a9d4b394613a25d7936b71f539c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ea7ef395c94aeb8b7497f5f8437ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df_data)\n",
    "validation_dataset = Dataset.from_pandas(val_df_data)\n",
    "test_dataset = Dataset.from_pandas(test_df_data)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset, \"test\": test_dataset})\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(template_dataset, remove_columns=list(dataset[\"train\"].features))\n",
    "\n",
    "print(train_dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "validation_dataset = dataset[\"validation\"].map(template_dataset, remove_columns=list(dataset[\"validation\"].features))\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(template_dataset, remove_columns=list(dataset[\"test\"].features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bae6c-a137-4a14-89d0-de7845e71c8f",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our inputs (text) to token IDs. This is done by a Hugging Face Transformers Tokenizer. In addition to QLoRA, we will use bitsanbytes 4-bit precision to quantize out frozen LLM to 4-bit and attach LoRA adapters on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9acb84c4-4217-404a-b724-51fc184e85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df960ec-bb4c-48c4-ab91-ae05bf03a34d",
   "metadata": {},
   "source": [
    "Utility method for finding the target modules and update the necessary matrices. Visit this link for additional info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdf82994-fdcd-481c-a1af-892188ff7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(hf_model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in hf_model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab506a3a-d005-4810-8618-184d1c548f65",
   "metadata": {},
   "source": [
    "### Define the Train Function and Execute it\n",
    "Train Function applies hyperparameters, and total GPU count, trains the model on the tokenized dataset, and runs an evaluation on the test dataset\n",
    "\n",
    "Evaluations will be run on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ed7c338-6f66-49a8-b8f6-7922dda013b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /home/sagemaker-user/AAI_540_SU_04/code\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.ImageUri\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.InstanceType\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from huggingface_hub import login\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sagemaker.remote_function import remote\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "\n",
    "# Start training\n",
    "@remote(volume_size=100, job_name_prefix=f\"train-{model_id.split('/')[-1].replace('.', '-')}-merge\")\n",
    "def train_fn(\n",
    "        model_name,\n",
    "        train_ds,\n",
    "        test_ds=None,\n",
    "        lora_r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=1,\n",
    "        chunk_size=2048,\n",
    "        gradient_checkpointing=False,\n",
    "        merge_weights=False,\n",
    "        token=None\n",
    "):  \n",
    "    print(\"############################################\")\n",
    "    print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "    print(\"############################################\")\n",
    "    \n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    if token is not None:\n",
    "        login(token=token)\n",
    "\n",
    "    # tokenize and chunk dataset\n",
    "    with accelerator.main_process_first():\n",
    "        lm_train_dataset = train_ds.map(\n",
    "            lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=per_device_train_batch_size, remove_columns=list(train_ds.features)\n",
    "        )\n",
    "\n",
    "    # Print total number of samples\n",
    "    print(f\"Total number of train samples: {len(lm_train_dataset)}\")\n",
    "\n",
    "\n",
    "    if test_ds is not None:\n",
    "        with accelerator.main_process_first():\n",
    "            lm_test_dataset = test_ds.map(\n",
    "                lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=per_device_eval_batch_size, remove_columns=list(test_ds.features)\n",
    "            )\n",
    "\n",
    "        print(f\"Total number of test samples: {len(lm_test_dataset)}\")\n",
    "    else:\n",
    "        lm_test_dataset = None\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=\"/tmp/.cache\"\n",
    "    )\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "    # get lora target modules\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    model = model.to(accelerator.device)\n",
    "\n",
    "    if test_ds is not None:\n",
    "        model, lm_train_dataset, lm_test_dataset = accelerator.prepare(\n",
    "            model, lm_train_dataset, lm_test_dataset\n",
    "        )\n",
    "    else:\n",
    "        model, lm_train_dataset = accelerator.prepare(\n",
    "            model, lm_train_dataset\n",
    "        )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=lm_train_dataset,\n",
    "        eval_dataset=lm_test_dataset if lm_test_dataset is not None else None,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            gradient_checkpointing=gradient_checkpointing,\n",
    "            logging_steps=2,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            bf16=True,\n",
    "            save_strategy=\"no\",\n",
    "            output_dir=\"outputs\"\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if merge_weights:\n",
    "        output_dir = \"/tmp/model\"\n",
    "\n",
    "        # merge adapter weights with base model and save\n",
    "        # save int 4 model\n",
    "        trainer.model.save_pretrained(output_dir, safe_serialization=False)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # load PEFT model in fp16\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            output_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=\"/tmp/.cache\"\n",
    "        )\n",
    "        \n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()\n",
    "        model.save_pretrained(\n",
    "            \"/opt/ml/model\", safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )\n",
    "    else:\n",
    "        model.save_pretrained(\"/opt/ml/model\", safe_serialization=True)\n",
    "\n",
    "    tmp_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tmp_tokenizer.save_pretrained(\"/opt/ml/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "316bbefe-7235-4680-ba1d-6102bc8a1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 07:34:03,834 sagemaker.remote_function INFO     Serializing function code to s3://sagemaker-us-east-1-114106928417/train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834/function\n",
      "2024-06-09 07:34:04,131 sagemaker.remote_function INFO     Serializing function arguments to s3://sagemaker-us-east-1-114106928417/train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834/arguments\n",
      "2024-06-09 07:34:04,496 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpioxnghqn/temp_workspace/sagemaker_remote_function_workspace/requirements.txt'\n",
      "2024-06-09 07:34:04,498 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmpioxnghqn/workspace.zip'\n",
      "2024-06-09 07:34:04,536 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-114106928417/train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834/sm_rf_user_ws/workspace.zip'\n",
      "2024-06-09 07:34:04,541 sagemaker.remote_function INFO     Creating job: train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::114106928417:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:114106928417:training-job/train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834 with an explicit deny in an identity-based policy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmerge_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/remote_function/client.py:323\u001b[0m, in \u001b[0;36mremote.<locals>._remote.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote function do not support training on multi instances. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide instance_count = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m RemoteExecutor\u001b[38;5;241m.\u001b[39m_validate_submit_args(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 323\u001b[0m job \u001b[38;5;241m=\u001b[39m \u001b[43m_Job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     job\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/remote_function/job.py:684\u001b[0m, in \u001b[0;36m_Job.start\u001b[0;34m(job_settings, func, func_args, func_kwargs, run_info)\u001b[0m\n\u001b[1;32m    673\u001b[0m training_job_request \u001b[38;5;241m=\u001b[39m _Job\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    674\u001b[0m     job_settings\u001b[38;5;241m=\u001b[39mjob_settings,\n\u001b[1;32m    675\u001b[0m     job_name\u001b[38;5;241m=\u001b[39mjob_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    680\u001b[0m     run_info\u001b[38;5;241m=\u001b[39mrun_info,\n\u001b[1;32m    681\u001b[0m )\n\u001b[1;32m    683\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating job: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[0;32m--> 684\u001b[0m \u001b[43mjob_settings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_job_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _Job(\n\u001b[1;32m    687\u001b[0m     job_name,\n\u001b[1;32m    688\u001b[0m     s3_base_uri,\n\u001b[1;32m    689\u001b[0m     job_settings\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    690\u001b[0m     training_job_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREMOTE_FUNCTION_SECRET_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    691\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::114106928417:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:114106928417:training-job/train-Mixtral-8x7B-Instruct-v0-1-merge-2024-06-09-07-34-03-834 with an explicit deny in an identity-based policy"
     ]
    }
   ],
   "source": [
    "train_fn(\n",
    "    model_id,\n",
    "    train_ds=train_dataset,\n",
    "    test_ds=test_dataset,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    merge_weights=True,\n",
    "    token=access_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c88aac-1f74-4238-982c-ce276df7a9d2",
   "metadata": {},
   "source": [
    "### Deploy Fine-Tuned model\n",
    "Note: Run train_fn with merge_weights=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81751683-8fb2-4109-b502-64d42a4694ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e597b-4e0d-46de-ba7d-3a2dadadbf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "job_prefix = f\"train-{model_id.split('/')[-1].replace('.', '-')}-merge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac4dd2-acb9-4173-8257-e6b4122ab52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefix):\n",
    "    import boto3\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    search_response = sagemaker_client.search(\n",
    "        Resource='TrainingJob',\n",
    "        SearchExpression={\n",
    "            'Filters': [\n",
    "                {\n",
    "                    'Name': 'TrainingJobName',\n",
    "                    'Operator': 'Contains',\n",
    "                    'Value': job_name_prefix\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'TrainingJobStatus',\n",
    "                    'Operator': 'Equals',\n",
    "                    'Value': \"Completed\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        SortBy='CreationTime',\n",
    "        SortOrder='Descending',\n",
    "        MaxResults=1)\n",
    "    \n",
    "    return search_response['Results'][0]['TrainingJob']['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d96415-513e-49c4-b890-e920ea159055",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = get_last_job_name(job_prefix)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3796719-a8c2-4a07-af84-478abf9d90c9",
   "metadata": {},
   "source": [
    "### Inference configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d7f87-43cf-4ccb-8372-bc21b6b80db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585b48c-f195-4d5d-9373-47b5c2014dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"1.4\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3561cb4-5dc5-4c0e-819c-01398f9bee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceModel(\n",
    "    image_uri=image_uri,\n",
    "    model_data=f\"s3://{bucket_name}/{job_name}/{job_name}/output/model.tar.gz\",\n",
    "    role=get_execution_role(),\n",
    "    env={\n",
    "        'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "        'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "        'HF_MODEL_QUANTIZE': \"bitsandbytes\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67e6ad-a7d5-4562-b660-cfcb12f40ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    model_data_download_timeout=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d0526-af45-425c-a47b-9e6d0e1d565a",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf9938-f51a-4284-97c1-08e424bd5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0819d-6be1-4a20-ba78-1336cd26946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<ENDPOINT_NAME>\" #Required if you want to create a predictor without running the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf1a4e-9280-495e-a56c-0d1f7f5db272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'predictor' not in locals() and 'predictor' not in globals():\n",
    "    print(\"Create predictor\")\n",
    "    predictor = HuggingFacePredictor(\n",
    "        endpoint_name=endpoint_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5cd87-5eaa-44ab-b2e9-41ec6346bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = f\"\"\"\n",
    "<s>[INST]\n",
    "{{question}} \n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ec682-a845-4b95-b5f9-77d4a1846f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = base_prompt.format(question=\"How do I make Windy City Wasabeans?\")\n",
    "\n",
    "predictor.predict({\n",
    "\t\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"n_predict\": -1,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842225b-e471-4577-9ba4-aad6130609d7",
   "metadata": {},
   "source": [
    "### Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9612a-0ab7-4ee9-9393-3982b906655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627c28a-1329-41ba-a220-a825464ccf48",
   "metadata": {},
   "source": [
    "### Shutdown Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9458b-1345-41a5-afc3-ca7c2a04fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dd378-c2ea-4399-b000-0f79e2a06dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c02ee-65c5-4b77-9692-f6d99a19756a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
